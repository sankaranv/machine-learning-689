\documentclass[11pt]{article}

\usepackage{fullpage}
\parindent=0in
\input{testpoints}

\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{color}

\newcommand{\argmax}{\mathop{\arg\max}}
\newcommand{\argmin}{\mathop{\arg\min}}

\newcommand{\deriv}[1]{\frac{\partial}{\partial {#1}} }
\newcommand{\dsep}{\mbox{dsep}}
\newcommand{\Pa}{\mathop{Pa}}
\newcommand{\ND}{\mbox{ND}}
\newcommand{\De}{\mbox{De}}
\newcommand{\Ch}{\mbox{Ch}}
\newcommand{\graphG}{{\mathcal{G}}}
\newcommand{\graphH}{{\mathcal{H}}}
\newcommand{\setA}{\mathcal{A}}
\newcommand{\setB}{\mathcal{B}}
\newcommand{\setS}{\mathcal{S}}
\newcommand{\setV}{\mathcal{V}}
\DeclareMathOperator*{\union}{\bigcup}
\DeclareMathOperator*{\intersection}{\bigcap}
\DeclareMathOperator*{\Val}{Val}
\newcommand{\mbf}[1]{{\mathbf{#1}}}
\newcommand{\eq}{\!=\!}


\begin{document}

{\centering
  \rule{6.3in}{2pt}
  \vspace{1em}
  {\Large
    CS689: Machine Learning - Fall 2019\\
    Homework 1\\
  }
  \vspace{1em}
  Assigned: Wednesday, Sept 11. Due: Wednesday, Sept 25 at 11:59pm \\
  \vspace{0.1em}
  \rule{6.3in}{1.5pt}
}
\vspace{1pc}

\textbf{Getting Started:} You should complete the assignment using your own installation of Python 3.6. Download the assignment archive from Moodle and unzip the file. The data files for this assignment are in the \verb|data| Directory. Code templates are in the \verb|code| directory. The only modules allowable during autograding are those already imported in the code templates.\\

\textbf{Deliverables:} This assignment has two types of deliverables: a report and code files.
\begin{itemize}
\item \textbf{Report: } The solution report will give your answers to the homework questions (listed below). The maximum length of the report is 5 pages in 11 point font, including all figures and tables. You can use any software to create your report, but your report must be submitted in PDF format. You will upload the PDF of your report to Gradescope under \verb|HW01-Report| for grading. Access to Gradescope will be enabled one week before the assignment is due. For full credit, all figures must have proper axes, labels, legends, and titles. Any tables must have proper row and/or column headings and titles or captions. You do \textbf{not} need to include the text of questions in your report.

\item \textbf{Code: } The second deliverable is your code. Your code must be Python 3.6 compatible (no iPython notebooks, other formats, or code from other versions of Python). You will upload a zip file (not rar, bz2 or other compressed format) containing all of your code to Gradescope under \verb|HW01-Programming| for autograding. Access to the autograder will be enabled one week before the assignment is due. When unzipped, your zip file should produce a directory called \verb|code|.  If your zip file has the wrong structure, the autograder may fail to run. To receive credit for implementation questions, your code must run in Gradescope. 
\end{itemize}

\textbf{Academic Honesty Reminder:} Copying solutions from external sources (books, web pages, etc.) or other students is considered cheating. Homework assignments are individual work. Sharing your solutions with other students is  considered cheating. Posting your code to public repositories like GitHub is also considered cheating. Collaboration indistinguishable from copying is considered cheating. Any detected cheating will result in a grade of 0 on the assignment for all students involved, and potentially a grade of F in the course. 
\\

\textbf{Questions:}

\begin{problem}{15} \textbf{Optimal Predictions for Zero-One Loss:} Suppose we have a probability distribution $P(Y=y, X=x)$ where $y\in\{0,1\}$ and $x\in\mathbb{R}$. Suppose we want to find the optimal prediction function $f(x)$ under the expected zero-one prediction loss criteria: $\mathbb{E}_{P(x,y)}[L_{01}(y,f(x))]$. Note that $f:\mathbb{R}\rightarrow \{0,1\}$. Use this information to answer the following questions.\\ 
	
\newpart{5}~\hspace{1em}  Provide an explicit mathematical expression for this optimization objective function in terms of sums and/or integrals, the distribution $P(X=x,Y=y)$ (and/or its marginals or conditionals), the prediction function $f(x)$, the loss functions $L$, and the inputs and outputs $x$ and $y$. \\
	
\newpart{10}~\hspace{1em} Prove that the function $f(x)$ that minimizes the expected loss $\mathbb{E}_{P(x,y)}[L_{01}(y,f(x))]$ is given by the expression below. Show your work and explain your solution.

 $$f(x)= \argmax_{y\in\{0,1\}} ~P(Y=y|X=x)$$ 

\end{problem}


\begin{problem}{40} \textbf{Logistic Regression with Centering:} In the standard binary logistic regression model, the probability of the class variable $y\in\{-1,1\}$ is modeled as a logistic function of the input $\mbf{x}$ and the parameters $\theta=[\mbf{w},b]$:  
	
	$$P(Y=y|\mbf{X}=\mbf{x},\theta) = \frac{1}{1+\exp(-y (\mbf{w}\mbf{x}^T + b))}$$
	
A common pre-processing approach when applying machine learning models is to pre-center the inputs so the mean on each dimension is 0. We can instead learn an optimal centering of the inputs by augmenting the logistic regression model with a vector of centering parameters $\mbf{c}$. The augmented parameter set is $\theta=[\mbf{w},\mbf{c}, b]$ and the augmented model is:

	$$P(Y=y|\mbf{X}=\mbf{x},\theta) = \frac{1}{1+\exp(-y (\mbf{w}(\mbf{x}-\mbf{c})^T + b))}$$

In this question, we will implement and analyze this augmented model. As the learning objective function, we will minimize the regularized negative conditional log likelihood:

$$\mathcal{L}(\mathcal{D},\theta) = \sum_{n=1}^N \log(1+\exp(-y_n(\mbf{w}(\mbf{x}_n-\mbf{c})^T+b))) + \lambda\Vert \mbf{w}\Vert_2^2 + \lambda\Vert \mbf{c}\Vert_2^2 +\lambda\cdot b^2$$


\newpart{5} Find the gradient of $\mathcal{L}(\mathcal{D},\theta)$ with respect to $\mbf{w}$, 
$\mbf{c}$, and $b$. Show your work.\\

\newpart{20} \hspace{0.5em} ~Starting from the provided template, implement a Scikit-Learn compatible class for this model. For learning, use the fmin\_l\_bfgs\_b optimizer from scipy.optimize with the disp=10 option to see output from the optimizer. Initialize all model parameters to 0. As your answer to this question, describe any issues you had implementing the model and what approach you used to fix them.\\

\newpart{5} Learn the model on the provided training data set with $\lambda=1e-6$, and evaluate it by computing its average prediction error on the test set. The entire learning procedure should take a few seconds. Report the average test error that you find as your answer to this question.\\

\newpart{5} Using the provided data, learn a standard logistic regression model using scikit-learn's linear\_model.LogisticRegression implementation with $C=10^6$ and solver='lbfgs'. Leave the other model hyper-parameters at their default values. Evaluate the standard model by computing its average prediction error on the test set. Report the average test error that you find as your answer to this question.\\

\newpart{5} If your implementation is correct, you should find that the augmented and standard models perform nearly identically. Explain why you think this is the case, providing as much detail as possible. 
\end{problem}


\begin{problem}{45} \textbf{Regression with Outliers:} One important problem with standard least-squares linear regression is that outliers can significantly corrupt learning. One way to combat this problem is with regression loss functions that are less sensitive to larger errors. Consider the robust prediction loss function shown below where $\delta>0$ is a parameter of the loss.
	
$$L_{\delta}(y,y')= \delta^2(\sqrt{1+(y-y')/\delta^2} - 1)$$

Using this robust loss for a linear regression model $f(\mbf{x}_n,\theta)=\mbf{w}\mbf{x}_n^T + b$
results in the learning objective
$$\mathcal{L}_{\delta}(\mathcal{D},\theta) = \sum_{n=1}^N L_{\delta}(y_n,f(\mbf{x}_n,\theta))$$

Use this model, objective function, and loss to answer the following questions. Assume that $\mbf{x}\in\mathbb{R}^D$ and $y\in\mathbb{R}$ in your derivations and code. \\

\newpart{5} Produce a plot of the robust loss function $L_{\delta}(y,y')$ for $\delta\in\{0.1, 1, 10\}$ where the x-axis corresponds to the prediction error $\epsilon=y-y'$ and the y-axis corresponds to the value of the loss. In the same figure, also include a similar plot of the standard squared loss $L_{sqr}(y,y')= (y-y')^2$. Use these plots to explain why the loss $L_{\delta}(y,y')$ could result in less sensitivity to outliers than the squared loss $L_{sqr}(y,y')$.\\

\newpart{5} Find the gradient of $\mathcal{L}_{\delta}(\mathcal{D},\theta)$ with respect to $\mbf{w}$ and $b$. Show your work.\\

\newpart{20} \hspace{0.5em} Starting from the provided template, implement a Scikit-Learn compatible class for this model. For learning, use the fmin\_l\_bfgs\_b optimizer from scipy.optimize with the disp=10 option to see output from the optimizer. Initialize all model parameters to 0. As your answer to this question, describe any issues you had implementing the model and what approach you used to fix them.
\\

\newpart{5} Using the provided data set, apply the model using $\delta=1$. Also apply Scikit-Learn's standard least-squares linear regression model linear\_model.LinearRegression. Report the MSE achieved by both models on the train data set.\\

\newpart{5} Provide a single figure that includes a scatter plot of the training data, the regression line found using the robust model (with $\delta=1$, and the regression line found using the standard least-squares model. \\

\newpart{5} Based on this plot, which model do you think would have better generalization performance on future test data? Explain your answer. \\

\end{problem}


\showpoints
\end{document}
